# -*- coding: utf-8 -*-
"""version3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ZyNXmD0mL_yLEu1KP-Mya-NWY0yKHcm

besoin de ces ficheirs:
* agriculture_data_required.csv
* historique_spectral_indices.csv
* polygon3.geojson
* terrain&spectral_data_NOgeol_for_regre.csv
* terrain_N045_or.csv

ne executer pas tous le code car y a des scripts prendre une heur d execution
"""

!pip install geojson
!pip install fanalysis
!pip install adjustText
!pip install pydataset
!pip install prince
!pip install --upgrade scikit-learn

import geojson
import ee
import pandas as pd

# Initialiser l'API Earth Engine
ee.Authenticate()
ee.Initialize(project='ee-wissalouh10')

# Importation des bibliothèques nécessaires
import ee
import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from fanalysis.pca import PCA
from adjustText import adjust_text
from ipywidgets import interact
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
from adjustText import adjust_text
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.metrics import silhouette_score, pairwise_distances
from scipy.spatial.distance import pdist
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from tabulate import tabulate
from datetime import datetime, timedelta
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from pydataset import data
# Options pour afficher les résultats clairement
sns.set(style="whitegrid")

"""# Partie 1
besoin d un seul fichier pour ce code ***agriculture_data_required.csv***
executer ***tout le code de la partie 1 normal ***
"""

data=pd.read_csv("/content/agriculture_data_required.csv")

# Affichage de la forme du jeu de données
# Objectif : Vérifier le nombre de lignes et de colonnes du jeu de données.
print("Shape of the dataset:", data.shape)

# Aperçu des 10 premières lignes
# Objectif : Examiner les premières lignes pour une compréhension initiale des données.
data.head(10)

# Vérification des valeurs manquantes
# Objectif : Identifier les colonnes avec des valeurs manquantes.
print("Missing values per column:")
print(data.isnull().sum())

# Distribution des étiquettes de cultures
# Objectif : Analyser la répartition des différentes cultures dans le jeu de données.
print("\nDistribution of crop labels:")
print(data["label"].value_counts())

# Calcul des moyennes des conditions agricoles
# Objectif : Fournir un résumé des moyennes des variables clés.
print("\nSummary of average conditions in the dataset:")
print("Average ratio of Nitrogen in Soil: {:.2f}".format(data["N"].mean()))
print("Average ratio of Phosphorus in Soil: {:.2f}".format(data["P"].mean()))
print("Average ratio of Potassium in Soil: {:.2f}".format(data["K"].mean()))
print("Average temperature (°C): {:.2f}".format(data["temperature"].mean()))
print("Average relative humidity (%): {:.2f}".format(data["humidity"].mean()))
print("Average pH value of the soil: {:.2f}".format(data["ph"].mean()))
print("Average rainfall (mm): {:.2f}".format(data["rainfall"].mean()))

# Résumé interactif pour une culture spécifique
# Objectif : Fournir des statistiques détaillées pour une culture donnée.
@interact
def summary(crops=list(data['label'].value_counts().index)):
    x = data[data['label'] == crops]
    print(f"\nStatistics for {crops}:")
    for condition in ["N", "P", "K", "temperature", "humidity", "ph", "rainfall"]:
        print(f"{condition.capitalize()} - Min: {x[condition].min()}, "
              f"Mean: {x[condition].mean():.2f}, Max: {x[condition].max()}")

# Comparaison des moyennes entre cultures et moyenne globale
# Objectif : Comparer les moyennes des conditions pour chaque culture.
@interact
def compare(conditions=['N', 'P', 'K', 'temperature', 'ph', 'humidity', 'rainfall']):
    global_mean = data[conditions].mean()
    print(f"\nAverage Value for {conditions}: {global_mean:.2f}")
    print("\nComparison with crops:")
    for crop in data['label'].unique():
        crop_mean = data[data['label'] == crop][conditions].mean()
        print(f"{crop}: {crop_mean:.2f}")

# Cultures avec des besoins au-dessus ou en-dessous de la moyenne
# Objectif : Identifier les cultures nécessitant des conditions supérieures ou inférieures à la moyenne.
@interact
def crop_requirements(conditions=['N', 'P', 'K', 'temperature', 'ph', 'humidity', 'rainfall']):
    print(f"\nCrops requiring above average {conditions}:")
    print(data[data[conditions] > data[conditions].mean()]['label'].unique())
    print(f"\nCrops requiring below average {conditions}:")
    print(data[data[conditions] <= data[conditions].mean()]['label'].unique())

# Distribution des variables dans une grille de sous-graphiques
# Objectif : Visualiser les distributions des variables dans le jeu de données.
variables = ['N', 'P', 'K', 'temperature', 'rainfall', 'humidity', 'ph']
n_vars = len(variables)
n_rows, n_cols = 3, 3  # Configuration de la grille

plt.figure(figsize=(15, 10))
for i, col in enumerate(variables):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(data=data, x=col, color=plt.cm.viridis(i / (n_vars - 1)))
    plt.xlabel(col.capitalize(), fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Distribution of {col.capitalize()}", fontsize=14)
    plt.grid(True)

plt.suptitle('Distribution of Agricultural Conditions', fontsize=20)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Objective: Analyzing the dataset to understand crop-specific requirements and trends.
print("## Interesting Facts About Crops ##\n")

# Thresholds for various attributes
thresholds = {
    'N': (120, "very High Ratio of Nitrogen Content in Soil"),
    'P': (100, "very High Ratio of Phosphorous Content in Soil"),
    'K': (200, "very High Ratio of Potassium Content in Soil"),
    'rainfall': (200, "very High Rainfall"),
    'temperature_low': (10, "very Low Temperature"),
    'temperature_high': (40, "very High Temperature"),
    'humidity_low': (20, "very Low Humidity"),
    'ph_low': (4, "very Low pH"),
    'ph_high': (9, "very High pH"),
}


# Displaying crop requirements based on thresholds
for key, (value, description) in thresholds.items():
    if 'low' in key:
        crops = data[data[key.split('_')[0]] < value]['label'].unique()
    else:
        crops = data[data[key.split('_')[0]] > value]['label'].unique()
    print(f"Crops requiring {description}: {crops}")

# Objective: Understanding the impact of seasonal conditions on crop suitability.
print("\n## Seasonal Crop Analysis ##\n")
seasonal_conditions = {
    'Summer Crops': (data['temperature'] > 30) & (data['humidity'] > 50),
    'Winter Crops': (data['temperature'] < 20) & (data['humidity'] > 30),
    'Rainy Crops': (data['rainfall'] > 200) & (data['humidity'] > 30),
}

for season, condition in seasonal_conditions.items():
    print(f"{season}: {data[condition]['label'].unique()}")

# SECTION 2: Clustering Analysis
# Objective: Group similar crops into clusters based on their attributes.
print("\n## Clustering Analysis Using K-Means ##\n")
x = data.drop(['label'], axis=1).values

# Determine the optimal number of clusters using the Elbow Method
wcss = []
for i in range(1, 11):
    km = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    km.fit(x)
    wcss.append(km.inertia_)

plt.figure(figsize=(10, 4))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('The Elbow Method', fontsize=15)
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# Apply K-Means with optimal clusters
optimal_clusters = 4
km = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)
y_means = km.fit_predict(x)

# Assign clusters to crops and display them
z = pd.concat([pd.DataFrame(y_means, columns=['cluster']), data['label']], axis=1)
for cluster in range(optimal_clusters):
    print(f"Crops in Cluster {cluster + 1}: {z[z['cluster'] == cluster]['label'].unique()}")

# SECTION 3: Predictive Modeling
# Objective: Build a predictive model to suggest the best crop for given conditions.
print("\n## Predictive Modeling Using Logistic Regression ##\n")

# Splitting the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(
    data.drop(['label'], axis=1), data['label'], test_size=0.2, random_state=0
)

print(f"x_train shape: {x_train.shape}, x_test shape: {x_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")

# Train logistic regression model
model = LogisticRegression(max_iter=200)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

# Evaluate model performance
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Test the model with a sample input
sample_input = np.array([[90, 40, 40, 20, 80, 7, 100]])
prediction = model.predict(sample_input)
print(f"The Suggested Crop for Given Climatic Condition is: {prediction[0]}")

print(model.predict(np.array([[90, 40, 40, 20, 80, 7, 100]])))

# SECTION 4: Data Integrity Check (Additional)
# Objective: Ensure data consistency and check for missing values
print("\n## Data Integrity Check ##\n")
missing_values = data.isnull().sum()
if missing_values.any():
    print("Missing Values Found:\n", missing_values)
else:
    print("No Missing Values Found. Data is clean.")

"""---



---

# partie 2
y a pas besoin d un fichier mais la creation de ces données prendre un temps d execution ***1H*** d acceder a toutes les images satellitaires disponible de ***25/07/2020 jusqu a 27/12/2023  (250 images)***
pour cela utilise le fichier ***historique_spectral_moyennes.csv*** (output de premier code)
"""

# Initialisation de Google Earth Engine
# Objectif : Préparer l'environnement pour accéder aux données Sentinel-2
ee.Initialize()

# Définir la Région d'Intérêt (ROI)
# Objectif : Spécifier la zone géographique pour laquelle les données seront récupérées
roi = ee.Geometry.Polygon(
    [[[-0.89731, 35.575695],
      [-0.897182, 35.575224],
      [-0.897074, 35.574744],
      [-0.896173, 35.574901],
      [-0.896817, 35.577606],
      [-0.897761, 35.577493],
      [-0.89731, 35.575695],
      [-0.89731, 35.575695]]]
)

# Fonction pour récupérer une image Sentinel-2 avec la couverture nuageuse minimale
# Objectif : Filtrer les images pour trouver la moins nuageuse dans une période donnée
def get_clear_sentinel2_image(start_date, end_date, roi):
    sentinel2 = ee.ImageCollection("COPERNICUS/S2")
    filtered_collection = sentinel2.filterBounds(roi).filterDate(start_date, end_date)
    sorted_collection = filtered_collection.sort('CLOUDY_PIXEL_PERCENTAGE')
    image = sorted_collection.first()
    return image.clip(roi) if image else None

# Fonction pour extraire les moyennes des bandes spectrales
# Objectif : Calculer la moyenne de chaque bande spectrale dans la ROI
def get_band_means(image, roi):
    bands = image.bandNames().getInfo()
    mean_dict = {}
    for band in bands:
        mean = image.select(band).reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=roi,
            scale=10,
            maxPixels=1e13
        ).get(band).getInfo()
        mean_dict[band] = mean if mean is not None else 0
    return mean_dict

# Objectif : Définir les paramètres d'extraction et préparer la structure des données
output_file = "historique_spectral_moyennes.csv"
start_date = datetime(2023, 12, 30)
days_back_if_found = 3
days_back_if_not_found = 1
data = []
all_bands = set()

"""!ne executez pas"""

# Recherche d'images et extraction des moyennes spectrales
# Objectif : Collecter les données spectrales sur plusieurs images
while len(data) < 250:
    end_date = start_date
    start_date -= timedelta(days=days_back_if_not_found)

    # Récupération de l'image avec couverture nuageuse minimale
    image = get_clear_sentinel2_image(start_date.isoformat(), end_date.isoformat(), roi)
    if not image:
        continue

    try:
        # Calcul des moyennes spectrales
        means = get_band_means(image, roi)
        means["date"] = end_date.isoformat()
        data.append(means)
        all_bands.update(means.keys())
        print(f"Image trouvée pour le {end_date.isoformat()}, données ajoutées.")
        start_date -= timedelta(days=days_back_if_found - days_back_if_not_found)
    except Exception as e:
        print(f"introuvable pour ce jour")
        continue

"""!ne executez pas

en cas que vous avez le executer importer ce fichier historique_spectral_indices.csv
"""

# Enregistrement des données dans un fichier CSV
# Objectif : Sauvegarder les données extraites pour une utilisation ultérieure
fieldnames = ["date"] + sorted(all_bands - {"date"})
with open(output_file, mode='w', newline='') as file:
    writer = csv.DictWriter(file, fieldnames=fieldnames)
    writer.writeheader()
    for row in data:
        filtered_row = {field: row.get(field, 0) for field in fieldnames}
        writer.writerow(filtered_row)
print(f"Extraction terminée. Données enregistrées dans {output_file}.") #output_file = "historique_spectral_moyennes.csv"

"""commancer a executer"""

output_file = "historique_spectral_moyennes.csv" #pour eviter d executer le code precedant

# Calcul des indices spectraux
# Objectif : Ajouter des indices dérivés des bandes spectrales aux données
data = pd.read_csv(output_file)
data["NDVI"] = (data["B8"] - data["B4"]) / (data["B8"] + data["B4"])
data["NDWI"] = (data["B3"] - data["B8"]) / (data["B3"] + data["B8"])
data["MNDWI"] = (data["B3"] - data["B11"]) / (data["B3"] + data["B11"])
data["NDRE"] = (data["B8A"] - data["B5"]) / (data["B8A"] + data["B5"])
data["PRI"] = (data["B2"] - data["B3"]) / (data["B2"] + data["B3"])
data["CIRE"] = (data["B8"] / data["B5"]) - 1
data["GNDVI"] = (data["B8"] - data["B3"]) / (data["B8"] + data["B3"])
data["SIPI"] = (data["B8"] - data["B4"]) / (data["B8"] - data["B2"])
data["LST"] = data["B10"]
data["NMDI"] = (data["B11"] - (data["B8A"] + data["B12"])) / (data["B11"] + (data["B8A"] + data["B12"]))

# Nettoyage des données
# Objectif : Supprimer les valeurs infinies et les données manquantes
data.replace([float("inf"), -float("inf")], 0, inplace=True)
data.fillna(0, inplace=True)

# Sauvegarde des données avec les indices spectraux
# Objectif : Exporter les données enrichies pour les analyses futures
output_file_indices = "historique_spectral_indices.csv"
data.to_csv(output_file_indices, index=False)
print(f"Les indices spectraux ont été calculés et sauvegardés dans {output_file_indices}.")

# Objectif : Charger et analyser les données spectrales issues du fichier CSV.
# Charger la base de données
data = pd.read_csv("/content/historique_spectral_indices.csv", parse_dates=["date"])
print(type(data))
print("Shape of the dataset:", data.shape)
print(data.head(10))

# Objectif : Convertir la colonne 'date' en type datetime pour des manipulations plus simples.
data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')  # Adaptez le format

# Objectif : Extraire les informations d'année, mois et jour à partir de la colonne 'date'.
data['année'] = data['date'].dt.year
data['mois'] = data['date'].dt.month
data['jour'] = data['date'].dt.day

# Vérification des nouvelles colonnes
print(data[['date', 'année', 'mois', 'jour']].head())

# Objectif : Supprimer la première colonne si elle est redondante.
data = data.drop(data.columns[0], axis=1)
print(data.head(10))

# Objectif : Identifier les colonnes contenant des valeurs nulles ou égales à zéro.
null_counts = data.isnull().sum()
print("Null value counts per column:", null_counts)

# Compter les colonnes avec NaN ou valeurs nulles
combined_counts = data.apply(lambda col: (col.isnull() | (col == 0)).sum())
print("\nNull and zero value counts per column:", combined_counts)

# Objectif : Supprimer les colonnes contenant au moins un NaN ou un 0.
cols_to_drop = data.columns[(data.isnull().any()) | ((data == 0).any())]
data = data.drop(cols_to_drop, axis=1)
print("Colonnes supprimées:", cols_to_drop)

# Vérification après suppression
combined_counts = data.apply(lambda col: (col.isnull() | (col == 0)).sum())
print("\nNull and zero value counts after cleanup:", combined_counts)

# Objectif : Sauvegarder les données nettoyées dans un fichier CSV.
data.to_csv("dataset_modifie.csv", index=False)

# Recharger et vérifier le fichier nettoyé
data = pd.read_csv("dataset_modifie.csv")
print(data.head(10))

# Objectif : Créer un sous-ensemble des colonnes pertinentes pour l'analyse.
colonnes_selectionnees = ['année', 'mois', 'jour', 'NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']
df = data[colonnes_selectionnees]
print(df.head())

# Objectif : Sauvegarder les données sélectionnées dans un nouveau fichier CSV.
df.to_csv("dataset_selectionnees.csv", index=False)
# Objectif : Charger le fichier sélectionné pour analyse.
df = pd.read_csv("/content/dataset_selectionnees.csv")

# Objectif : Analyser les indices globaux et par période.
indices = ['NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']

# Description globale des indices
global_description = df[indices].describe()
print("Statistiques globales :", global_description)

# Description par année
description_by_year = df.groupby('année')[indices].describe()
print("\nStatistiques par année :", description_by_year)

# Description par mois
description_by_month = df.groupby('mois')[indices].describe()
print("\nStatistiques par mois :", description_by_month)

# Objectif : Sauvegarder les statistiques dans des fichiers CSV.
global_description.to_csv("description_globale.csv", index=False)
description_by_year.to_csv("description_par_annee.csv", index=False)
description_by_month.to_csv("description_par_mois.csv", index=False) #need this one after

# Objectif : Créer des visualisations détaillées pour chaque indice, en analysant les tendances mensuelles, les distributions et les corrélations.

def visualiser_indices_mensuels(df, indices, mois_colonne='mois', titre="Visualisations Mensuelles des Indices"):
    """
    Crée des visualisations détaillées (boxplots, violon plots, tendances, heatmaps) pour analyser les indices mensuellement.

    Args:
        df (pd.DataFrame): DataFrame contenant les données des indices et des mois.
        indices (list): Liste des noms de colonnes correspondant aux indices.
        mois_colonne (str): Nom de la colonne contenant les mois (par défaut 'mois').
        titre (str): Titre général des graphiques.
    """
    if mois_colonne not in df.columns:
        raise ValueError(f"La colonne '{mois_colonne}' est manquante dans le DataFrame.")

    for indice in indices:
        if indice not in df.columns:
            print(f"Avertissement : L'indice '{indice}' est absent du DataFrame.")
            continue

        # Configuration de la figure
        plt.figure(figsize=(15, 10))

        # Étape 1 : Analyse de la distribution mensuelle avec des boxplots
        plt.subplot(2, 2, 1)
        sns.boxplot(x=mois_colonne, y=indice, data=df, palette="viridis")
        plt.title(f"Boxplot de {indice} par Mois")
        plt.xticks(rotation=45)

        # Étape 2 : Analyse de la densité avec des violon plots
        plt.subplot(2, 2, 2)
        sns.violinplot(x=mois_colonne, y=indice, data=df, palette="viridis", inner="quartile")
        plt.title(f"Violin Plot de {indice} par Mois")
        plt.xticks(rotation=45)

        # Étape 3 : Identification des tendances avec un graphique linéaire
        plt.subplot(2, 2, 3)
        df.groupby(mois_colonne)[indice].mean().plot(marker='o', color='blue')
        plt.title(f"Moyenne de {indice} par Mois")
        plt.xticks(rotation=45)
        plt.ylabel(indice)

        # Étape 4 : Exploration avec une heatmap (valeurs agrégées mensuelles)
        try:
            pivot_df = df.pivot_table(index=mois_colonne, values=indice, aggfunc='mean')
            plt.subplot(2, 2, 4)
            sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f")
            plt.title(f"Heatmap de {indice} par Mois")
        except ValueError as e:
            print(f"Avertissement : {e}")

        plt.suptitle(titre, fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.show()

# Appel des fonctions
indices_exemples = ['NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']

# Exemple pour visualiser chaque indice individuellement
visualiser_indices_mensuels(df, indices_exemples)

"""# ***Interprétation des graphiques pour l'indice NDWI :***
# **Graphique 1 : Boxplot de NDWI par mois**
Interprétation : Le boxplot montre la distribution des valeurs de NDWI pour chaque mois. Les médianes (ligne dans chaque boîte) indiquent une tendance générale du NDWI à travers les mois. Par exemple, les mois 2, 3, et 12 montrent une variation relativement plus large, suggérant une forte variabilité des conditions environnementales ou des données d'eau pour ces mois. Les outliers (points isolés) sont visibles, surtout pour le mois 6, ce qui indique des valeurs extrêmes ou anomalies dans les données.

#***Graphique 2 : Violin Plot de NDWI par mois***
Le violin plot montre à la fois la distribution et la densité des valeurs de NDWI pour chaque mois. Les mois 4, 5, et 6 présentent une densité plus concentrée autour de valeurs spécifiques, ce qui signifie moins de dispersion dans les données. Les mois 2, 3, et 12 ont des distributions plus larges, avec des pics multiples, ce qui correspond à des variations ou à des conditions différentes dans ces périodes.

# ***Graphique 3 : Moyenne de NDWI par mois***
Interprétation : La courbe montre les moyennes mensuelles de NDWI, révélant une tendance générale au cours de l'année. Une augmentation significative est observée entre les mois 3 et 5, atteignant un pic au mois 6. Cela peut correspondre à des saisons avec plus de disponibilité en eau (ex. printemps ou début d'été). Une diminution rapide se produit entre les mois 6 et 8, suggérant une sécheresse ou une diminution de la végétation, avant de stabiliser dans les mois suivants.

# ***Graphique 4 : Heatmap de NDWI par mois***
Interprétation : La heatmap fournit une vue simplifiée des variations de NDWI par mois. Les valeurs les plus faibles de NDWI (couleurs sombres, autour de -0.3) sont observées pendant les mois d'été (8 notamment). Les valeurs les plus élevées (couleurs plus claires) se situent entre les mois 4 et 6, confirmant une période favorable à la présence d'eau ou à la végétation active.

# ***Conclusion Générale :***
L'indice NDWI montre une variabilité saisonnière marquée. Période printanière (mois 4 à 6) : Les valeurs augmentent, indiquant potentiellement une augmentation des ressources en eau ou de l'humidité dans le sol/végétation. Période estivale (mois 7 à 9) : Les valeurs chutent rapidement, ce qui pourrait refléter une sécheresse ou une réduction de la couverture végétale.

# ***l objectif***
Les variations entre les mois montrent l'importance de la saisonnalité et des conditions climatiques/environnementales dans l'évolution des indices.
"""

# Objectif : Visualiser simultanément les indices mensuels avec des axes Y distincts pour une meilleure lisibilité.

def visualiser_indices_avec_axes_y_sep(df, indices, indice_lst='LST', mois_colonne='mois', titre="Visualisation Mensuelle de Tous les Indices"):
    """
    Visualise les indices mensuels sur deux axes Y : un pour l'indice LST et l'autre pour les autres indices.

    Args:
        df (pd.DataFrame): DataFrame contenant les données.
        indices (list): Liste des colonnes correspondant aux indices.
        indice_lst (str): Colonne représentant LST (température de surface).
        mois_colonne (str): Colonne contenant les mois.
        titre (str): Titre général du graphique.
    """
    if mois_colonne not in df.columns:
        raise ValueError(f"La colonne '{mois_colonne}' est absente du DataFrame.")

    indices_existants = [indice for indice in indices if indice in df.columns]
    if indice_lst not in indices_existants:
        raise ValueError(f"L'indice LST '{indice_lst}' est manquant dans le DataFrame.")

    autres_indices = [indice for indice in indices_existants if indice != indice_lst]
    if not autres_indices:
        raise ValueError("Aucun autre indice trouvé à part LST.")

    moyennes_mensuelles = df.groupby(mois_colonne).mean()

    # Configuration de la figure et des axes
    fig, ax1 = plt.subplots(figsize=(15, 8))

    # Étape 1 : Affichage de LST sur le premier axe Y
    ax1.plot(moyennes_mensuelles.index, moyennes_mensuelles[indice_lst], color='red', marker='o', label=indice_lst, linewidth=2)
    ax1.set_ylabel(f"Valeurs de {indice_lst}", color='red')
    ax1.tick_params(axis='y', labelcolor='red')
    ax1.set_xlabel("Mois")
    ax1.grid(axis='y', linestyle='--', alpha=0.7)

    # Étape 2 : Affichage des autres indices sur le second axe Y
    ax2 = ax1.twinx()
    for indice in autres_indices:
        ax2.plot(moyennes_mensuelles.index, moyennes_mensuelles[indice], marker='o', label=indice)
    ax2.set_ylabel("Valeurs des autres indices")
    ax2.tick_params(axis='y')
    ax2.legend(loc='upper left', bbox_to_anchor=(1.05, 1), title="Indices")

    # Finalisation
    plt.title(titre, fontsize=16)
    plt.xticks(moyennes_mensuelles.index, moyennes_mensuelles.index, rotation=45)
    fig.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

# Exemple pour une visualisation combinée avec deux axes Y
visualiser_indices_avec_axes_y_sep(df, indices_exemples, indice_lst='LST', titre="Indices Mensuels avec Deux Axes Y")

"""# ***Analyse globale des tendances mensuelles***
Température de surface (LST) (axe Y gauche, en rouge) :

La température de surface atteint son pic en mars et juin, ce qui correspond probablement aux périodes de chaleur intense ou de saison sèche. Une baisse significative de la LST est observée en juillet-août, probablement liée à des précipitations ou une augmentation de l'humidité (début d'une saison humide). Autres indices (axe Y droit, courbes multiples) :

Les indices environnementaux (NDVI, NDWI, MNDWI, NDRE, etc.) suivent des variations plus stables sur l'année, mais certains montrent des changements significatifs : NDWI (eau) : Pic en mai-juin, indiquant une saison humide ou une recharge en eau. Baisse en juillet-août, typique d'une période sèche. NDVI (végétation) : Sa valeur reste stable sur plusieurs mois, mais un léger creux est visible en juillet, ce qui peut refléter un stress hydrique ou une saison sèche. CIRE et NDRE : Ces indices (liés à la santé de la végétation) montrent des variations modérées, avec un pic en fin d'année (octobre-décembre). Cela pourrait indiquer une récupération de la végétation.
"""

# Objectif : Préparer les données pour l'ACP en regroupant par jour et mois et en calculant les moyennes
dff_grouped = df.groupby(['jour', 'mois']).mean().reset_index()

# Suppression des colonnes inutiles pour l'ACP
# Objectif : Retirer les colonnes non pertinentes comme "année", "mois", et "jour"
dff = dff_grouped.drop(df.columns[[0, 1, 2]], axis=1)

# Préparation des données pour l'ACP (suppression des colonnes non numériques)
# Objectif : S'assurer que seules les colonnes numériques sont utilisées dans l'ACP
dff = dff.select_dtypes(include=np.number)

# 1. Exploration des données avant ACP
# Objectif : Analyser les données et comprendre leur distribution et les corrélations
print("Description des données avant ACP :\n", dff.describe())

print("Corrélations entre les variables avant ACP :\n", dff.corr())
sns.heatmap(dff.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matrice de Corrélation des Variables Avant ACP")
plt.show()

# 2. Initialisation et ajustement de l'ACP
# Objectif : Réduire la dimensionnalité tout en capturant le maximum de variance
acp = PCA(
    std_unit=True,
    stats=True,
    col_labels=dff.columns,
    row_labels=dff_grouped.apply(lambda x: f"{x['jour']}/{x['mois']}", axis=1).values
)
acp.fit(dff.values)

# Extraction des données après l'ACP
coord_indiv = acp.row_topandas()[['row_coord_dim1', 'row_coord_dim2']].values
coord_var = acp.col_topandas()[['col_coord_dim1', 'col_coord_dim2']]
explained_variance = acp.eig_[1]  # Variance expliquée en pourcentage

# Affichage des éboulis des valeurs propres
# Objectif : Identifier les dimensions principales
acp.plot_eigenvalues(type="percentage")

# 3. Analyse des variables après l'ACP
# Objectif : Comprendre la contribution et la qualité des variables sur les axes
info_var = acp.col_topandas()
cos2_var = info_var[['col_cos2_dim1', 'col_cos2_dim2']]
contrib_var = info_var[['col_contrib_dim1', 'col_contrib_dim2']]
print("Cos2 des variables :\n", cos2_var)
print("Contributions des variables :\n", contrib_var)

# 4. Visualisation du cercle des corrélations
# Objectif : Représenter graphiquement les corrélations entre les variables
acp.correlation_circle(num_x_axis=1, num_y_axis=2)

# 5. Analyse des individus après l'ACP
# Objectif : Comprendre la qualité de représentation des individus sur les axes
info_indiv = acp.row_topandas()
cos2_indiv = info_indiv[['row_cos2_dim1', 'row_cos2_dim2']]
contrib_indiv = info_indiv[['row_contrib_dim1', 'row_contrib_dim2']]
print("Cos2 des individus :\n", cos2_indiv)
print("Contributions des individus :\n", contrib_indiv)

# 6. Visualisation combinée (individus et variables)
# Objectif : Représenter les individus et les variables dans le plan factoriel
x_range = np.ptp(coord_indiv[:, 0])
y_range = np.ptp(coord_indiv[:, 1])
u_range = np.ptp(coord_var['col_coord_dim1'])
v_range = np.ptp(coord_var['col_coord_dim2'])
scaling_factor = min(x_range / u_range, y_range / v_range) / 3

plt.figure(figsize=(16, 12))
texts = []

# Ajout des individus
for i, (x, y) in enumerate(coord_indiv):
    jour = dff_grouped['jour'].iloc[i]
    mois = dff_grouped['mois'].iloc[i]
    plt.scatter(x, y, s=100, alpha=0.7, label=f"Jour {jour}, Mois {mois}")
    texts.append(plt.text(x, y, str(acp.row_labels[i]), fontsize=10, ha='center', va='center'))

# Ajout des vecteurs des variables
for i, (x, y) in coord_var.iterrows():
    x = x * scaling_factor
    y = y * scaling_factor
    plt.arrow(0, 0, x, y, color='blue', alpha=0.8, head_width=0.04 * scaling_factor, length_includes_head=True)
    plt.text(x * 1.1, y * 1.1, i, color='blue', ha='center', va='center', fontsize=12, fontweight='bold')

adjust_text(texts, autoalign='xy', only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle="-", color='black', lw=0.5))

# Ajout du cercle de corrélation
circle = plt.Circle((0, 0), scaling_factor, color='gray', fill=False, ls='--', lw=1.5)
plt.gca().add_artist(circle)

# Personnalisation de l'affichage
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.axvline(0, color='black', linewidth=0.8, linestyle='--')
plt.xlabel(f"Dimension 1 ({explained_variance[0]}%)", fontsize=14, fontweight='bold')
plt.ylabel(f"Dimension 2 ({explained_variance[1]}%)", fontsize=14, fontweight='bold')
plt.title("Analyse en Composantes Principales (ACP) - Combinaison Individus et Variables", fontsize=16, fontweight='bold')
plt.grid(True, linestyle=':', alpha=0.6)
plt.axis('equal')
plt.tight_layout()
plt.show()

# 7. Analyse des contributions graphiques
# Objectif : Identifier les variables et individus les plus représentatifs
acp.plot_row_contrib(num_axis=1, nb_values=7, figsize=(5, 3))
acp.plot_row_cos2(num_axis=1, nb_values=7, figsize=(5, 3))
acp.plot_col_contrib(num_axis=1, figsize=(5, 3))
acp.plot_col_cos2(num_axis=1, figsize=(5, 3))

# Étape 1 : Regroupement des données par mois
# Objectif : Calculer les moyennes mensuelles pour simplifier les données
dff_grouped = df.groupby(['mois']).mean().reset_index()
dff_grouped['mois'] = dff_grouped['mois'].astype(int)

# Étape 2 : Préparation des données pour l'ACP
# Objectif : Supprimer les colonnes non pertinentes et conserver uniquement les données numériques
dff = dff_grouped.drop(columns=['année', 'jour', 'mois'])
dff = dff.select_dtypes(include=np.number)

# Étape 3 : Analyse descriptive des données
# Objectif : Examiner les statistiques descriptives et les corrélations avant l'ACP
print("Description des données avant ACP :\n", dff.describe())

print("Corrélations entre les variables avant ACP :\n", dff.corr())
sns.heatmap(dff.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matrice de Corrélation des Variables Avant ACP")
plt.show()

# Étape 4 : Initialisation et ajustement de l'ACP
# Objectif : Identifier les composantes principales
acp = PCA(
    std_unit=True,  # Normalisation des données
    stats=True,  # Calcul des statistiques
    col_labels=dff.columns,
    row_labels=dff_grouped['mois'].astype(str).values
)
acp.fit(dff.values)

# Étape 5 : Extraction des résultats de l'ACP
# Objectif : Extraire les coordonnées des individus et des variables
coord_indiv = acp.row_topandas()[['row_coord_dim1', 'row_coord_dim2']].values
coord_var = acp.col_topandas()[['col_coord_dim1', 'col_coord_dim2']]
explained_variance = acp.eig_[1]  # Pourcentage de variance expliquée

# Étape 6 : Facteur d'échelle pour les graphiques
# Objectif : Ajuster les vecteurs des variables pour éviter le chevauchement
x_range = np.ptp(coord_indiv[:, 0])
y_range = np.ptp(coord_indiv[:, 1])
u_range = np.ptp(coord_var['col_coord_dim1'])
v_range = np.ptp(coord_var['col_coord_dim2'])
scaling_factor = min(x_range / u_range, y_range / v_range) / 3

# Étape 7 : Visualisation combinée (individus et variables)
plt.figure(figsize=(16, 12))
texts = []
# Points des individus
for i, (x, y) in enumerate(coord_indiv):
    mois = dff_grouped['mois'].iloc[i]
    plt.scatter(x, y, s=100, alpha=0.7, label=f"Mois {mois}")
    texts.append(plt.text(x, y, str(mois), fontsize=10, ha='center', va='center'))

# Cercle de corrélation
circle = plt.Circle((0, 0), scaling_factor, color='gray', fill=False, ls='--', lw=1.5)
plt.gca().add_artist(circle)

# Vecteurs des variables
for i, (x, y) in coord_var.iterrows():
    x, y = x * scaling_factor, y * scaling_factor
    plt.arrow(0, 0, x, y, color='blue', alpha=0.8, head_width=0.04 * scaling_factor, length_includes_head=True)
    plt.text(x * 1.1, y * 1.1, i, color='blue', ha='center', va='center', fontsize=12, fontweight='bold')

# Ajustement du texte
adjust_text(texts, autoalign='xy', only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle="-", color='black', lw=0.5))

# Esthétique du graphique
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.axvline(0, color='black', linewidth=0.8, linestyle='--')
plt.xlabel(f"Dimension 1 ({explained_variance[0]:.2f}%)", fontsize=14, fontweight='bold')
plt.ylabel(f"Dimension 2 ({explained_variance[1]:.2f}%)", fontsize=14, fontweight='bold')
plt.title("ACP - Individus et Variables", fontsize=16, fontweight='bold')
plt.grid(True, linestyle=':', alpha=0.6)
plt.axis('equal')
plt.tight_layout()
plt.show()

# Étape 1: Préparation des données de Clustering
# Regroupement des données par mois et calcul des moyennes
df_grouped = df.groupby(['mois']).mean().reset_index()
df_grouped['mois'] = df_grouped['mois'].astype(int)

# Suppression des colonnes non nécessaires pour l'analyse
data2 = df_grouped.drop(columns=['année', 'jour', 'mois'])

# Affichage d'un aperçu des données
print("Aperçu des données:")
print(data2.head())
print("\nStatistiques descriptives:")
print(data2.describe())

# Étape 2: Standardisation des données
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data2)

# Étape 3: Clustering hiérarchique et visualisation
# Agglomération hiérarchique pour initialiser le regroupement
hc = AgglomerativeClustering(n_clusters=4, metric='euclidean', linkage='ward')
hc_labels = hc.fit_predict(data_scaled)

# Création et affichage du dendrogramme
dist_matrix = pdist(data_scaled, metric='euclidean')
linkage_matrix = linkage(dist_matrix, method='ward')
plt.figure(figsize=(14, 8))
dendrogram(linkage_matrix, labels=data2.index, leaf_rotation=90)
plt.title("Dendrogramme des Indices")
plt.xlabel("Indices")
plt.ylabel("Distance Euclidienne")
plt.show()

# Étape 4: Réduction de dimension avec PCA
pca = PCA()
res_pca = pca.fit_transform(data_scaled)

# Étape 5: Clustering avec KMeans
# Calcul du meilleur nombre de clusters en utilisant l'indice de silhouette
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(data_scaled)
    silhouette_scores.append(silhouette_score(data_scaled, kmeans.labels_))

best_k = silhouette_scores.index(max(silhouette_scores)) + 2
print(f"Meilleur nombre de clusters selon l'indice de silhouette: {best_k}")

# Affichage des scores de silhouette
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')
plt.title("Indice de silhouette pour différents nombres de clusters")
plt.xlabel("Nombre de clusters")
plt.ylabel("Score de silhouette")
plt.show()

#peut usiliser 3 car a la meme score de silhouette que 5 et pour notre etude 3 c est preferable car sont des mois d agriculture
best_k=3

# Clustering final avec le meilleur nombre de clusters
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(data_scaled)

# Étape 6: Visualisation des clusters
plt.figure(figsize=(7, 5))
plt.scatter(res_pca[:, 0], res_pca[:, 1], c=kmeans_labels, cmap='viridis', marker='o')
plt.xlabel('Composante 1')
plt.ylabel('Composante 2')
plt.title('Carte Factorielle avec le Meilleur Nombre de Clusters')
for i, label in enumerate(df_grouped['mois']):
    plt.annotate(label, (res_pca[i, 0], res_pca[i, 1]), ha='right', va='bottom')
plt.show()

# Étape 7: Analyse des clusters
cluster_summary = {}
for i in range(3):
    cluster_indices = np.where(kmeans_labels == i)[0]
    cluster_months = df_grouped['mois'].iloc[cluster_indices]
    cluster_data = data2.iloc[cluster_indices]
    cluster_summary[i] = {
        "Nombre de mois": len(cluster_months),
        "Mois": cluster_months.tolist(),
        "Statistiques descriptives": cluster_data.describe()
    }
    print(f"\nCluster {i}:")
    print(f"Nombre de mois: {len(cluster_months)}")
    print(f"Mois: {cluster_months.tolist()}")
    print(cluster_data.describe())

# Étape 8: Points centraux des clusters
centroids = kmeans.cluster_centers_
distances = pairwise_distances(data_scaled, centroids)

closest_indices = distances.argmin(axis=0)
closest_points = df_grouped['mois'].iloc[closest_indices]
for i, point in enumerate(closest_points):
    print(f"Cluster {i}: {point} est le mois le plus proche du centroïde.")

"""# ***regrouper les resultats pour l interpretation***"""

# Standardisation des données
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data2)

# PCA pour réduire les dimensions à 2
pca = PCA(n_components=2)
res_pca = pca.fit_transform(data_scaled)
explained_variance = pca.explained_variance_ratio_ * 100

# Clustering KMeans
best_k = 3  # Choix optimal pour KMeans
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(data_scaled)
centroids = kmeans.cluster_centers_

# Coordonnées des variables pour le cercle de corrélation
coord_var = pca.components_.T  # Colonnes des coordonnées des variables

# Mise à l'échelle des vecteurs pour qu'ils rentrent dans le cercle
scaling_factor = min(
    np.ptp(res_pca[:, 0]) / np.ptp(coord_var[:, 0]),
    np.ptp(res_pca[:, 1]) / np.ptp(coord_var[:, 1])
)
coord_var *= scaling_factor

# Création du graphique
plt.figure(figsize=(14, 10))

# Cercle de corrélation
circle = plt.Circle((0, 0), 1 * scaling_factor, color='gray', fill=False, linestyle='--', linewidth=1.5)
plt.gca().add_artist(circle)

# Vecteurs des variables
for i, (x, y) in enumerate(coord_var):
    plt.arrow(0, 0, x, y, color='blue', alpha=0.8, head_width=0.05 * scaling_factor, length_includes_head=True)
    # Remplacement par les noms des colonnes de data2
    plt.text(x * 1.1, y * 1.1, data2.columns[i], color='blue', ha='center', va='center', fontsize=12, fontweight='bold')

# Points des observations
scatter = plt.scatter(
    res_pca[:, 0], res_pca[:, 1], c=kmeans_labels, cmap='tab10', s=100, alpha=0.8, edgecolor='white', linewidth=0.6
)

# Centroides
for i, (cx, cy) in enumerate(pca.transform(centroids)):
    plt.scatter(cx, cy, marker='X', s=200, c='red', edgecolor='black', linewidth=1.5, label=f'Centroid Cluster {i}')

# Texte des points (Observations)
texts = []
for i, (x, y) in enumerate(res_pca):
    mois = dff_grouped['mois'].iloc[i]
    texts.append(plt.text(x, y, f'Mois {mois}', fontsize=10, ha='center', va='center'))

# Ajustement des textes pour éviter le chevauchement
adjust_text(
    texts,
    arrowprops=dict(arrowstyle='-', color='black', lw=0.5),
    expand_text=(1.1, 1.5),
    expand_points=(1.2, 1.5),
    force_text=(0.6, 0.6)
)

# Axes et étiquettes
plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
plt.axvline(0, color='black', linewidth=0.8, linestyle='--')
plt.xlabel(f"Dimension 1 ({explained_variance[0]:.2f}%)", fontsize=14, fontweight='bold')
plt.ylabel(f"Dimension 2 ({explained_variance[1]:.2f}%)", fontsize=14, fontweight='bold')
plt.title("Cercle de Corrélation et Observations avec Clusters", fontsize=16, fontweight='bold', pad=20)
plt.grid(True, linestyle=':', alpha=0.6)

# Légende et esthétique
plt.legend(loc='best', fontsize=12, frameon=True, shadow=True)
plt.tight_layout()
plt.show()

"""Cercle de corrélation (gris pointillé) : Représente les corrélations entre les variables et les axes principaux. Variables (flèches bleues) : Indiquent la direction et l'intensité de la contribution de chaque variable aux axes principaux. Observations (points colorés - Mois) : Représentent les mois, colorés selon les 3 clusters. Axes principaux (Dimensions 1 avec 50.80% et 2 avec 29.29%) : Expliquent la variance des données. Interprétation détaillée :

Dimension 1 (Axe Horizontal) :

Cette dimension semble principalement opposer les variables NDVI, NDRE et GNDVI (pointant vers la droite) aux variables MNDWI et PRI (pointant vers le haut). Les mois situés à droite du graphique (Mois 1,2, Mois 3, Mois 12) ont des valeurs plus élevées pour NDVI, NDRE et GNDVI, et des valeurs plus basses pour MNDWI et PRI. Les mois situés à gauche du graphique (Mois 6, Mois 5,4, Mois 9, Mois 10) ont la tendance inverse. Dimension 2 (Axe Vertical) :

Cette dimension semble principalement liée à la variable NMMDI (pointant vers le bas) et, dans une moindre mesure, à LST (pointant vers le haut). Le Mois 8, situé en bas du graphique, se distingue par une valeur élevée pour NMMDI. Les Mois 11 et 12, situés en haut du graphique, ont des valeurs plus élevées pour LST. Corrélation entre les variables :

Corrélations positives fortes : NDVI, NDRE et GNDVI sont très fortement corrélés positivement entre eux (flèches presque superposées). Cela est logique, car ce sont des indices de végétation qui mesurent des aspects similaires de la biomasse et de l'activité photosynthétique. MNDWI et PRI semblent également corrélés positivement, bien que moins fortement que les indices de végétation. Corrélations négatives : NDVI, NDRE et GNDVI sont corrélés négativement avec MNDWI et PRI (flèches dans des directions opposées). Cela suggère que lorsque les indices de végétation sont élevés, les valeurs de MNDWI et PRI ont tendance à être basses, et vice-versa. Corrélations faibles ou nulles : NMMDI et LST semblent peu corrélés avec les autres variables, car leurs flèches forment des angles proches de 90 degrés avec les autres flèches. Structure des Clusters (Mois) :

Cluster 1 (Vert/Turquoise) : Mois 4,5, 6, 9, et 10. Ces mois présentent des valeurs intermédiaires pour la plupart des variables, avec une légère tendance à des valeurs plus élevées pour NDWI. Cluster 2 (Violet) : Mois 1,2, 3, et 12. Ces mois se caractérisent par des valeurs élevées pour NDVI, NDRE et GNDVI, et des valeurs basses pour MNDWI et PRI. On peut les interpréter comme des mois de forte activité végétative. Cluster 3 (Jaune) : Mois 7 et 8. Ces mois se distinguent par des valeurs plus élevées pour NMMDI. Le mois 8 se distingue particulièrement avec une forte valeur de NMMDI et des faibles valeurs pour les indices de végétation. Interprétation Globale et Hypothèses :

Les deux premières dimensions de l'ACP capturent une part importante de la variabilité des données. La Dimension 1 reflète un gradient lié à l'activité de la végétation, opposant les mois avec une forte biomasse (NDVI, NDRE, GNDVI élevés) aux mois avec d'autres caractéristiques (MNDWI, PRI élevés). La Dimension 2 semble être liée à un autre facteur, potentiellement l'humidité ou la température (NMMDI et LST). Les clusters de mois pourraient refléter des variations saisonnières, des conditions climatiques différentes, ou des stades de développement de la végétation. Par exemple, le Cluster 2 (Mois 1, 3, 12) pourrait correspondre à une période de croissance végétative active, tandis que le Cluster 3 (Mois 7, 8) pourrait correspondre à une période plus sèche ou avec une végétation moins développée.

L'idée est de chercher des liens entre les variations des indices de végétation (NDVI, NDRE, GNDVI, NDWI, NMMDI) et les conditions environnementales (température, humidité) et les nutriments du sol (M, P, K) on peut établir des relations générales et formuler des hypothèses: Relations Générales et Hypothèses :

NDVI, NDRE, GNDVI (Indices de Végétation) :

Température : Généralement, une température optimale favorise une forte activité photosynthétique et donc des valeurs élevées de ces indices. Des températures trop basses ou trop élevées peuvent limiter la croissance et réduire les indices. Humidité : Une disponibilité en eau suffisante est cruciale pour la croissance des plantes. Un manque d'eau (sécheresse) réduit les indices, tandis qu'un excès peut également être néfaste dans certains cas. Nutriments (M, P, K) : Une carence en nutriments essentiels (macro-éléments : azote (N), phosphore (P), potassium (K) et les méso-éléments : calcium (Ca), magnésium (Mg), soufre (S)) peut limiter la croissance et réduire les indices. Un apport équilibré favorise une végétation saine et des indices élevés. NDWI (Indice d'Eau) :

Humidité : Cet indice est directement lié à la teneur en eau des plantes et du sol. Des valeurs élevées indiquent une bonne disponibilité en eau. Température : La température influence l'évapotranspiration et donc la disponibilité en eau pour les plantes. Des températures élevées peuvent entraîner un stress hydrique même si l'humidité du sol est initialement suffisante. NMMDI (Indice Modifié de Différence d'Humidité Normalisée) :

Humidité : Cet indice est souvent utilisé pour détecter le stress hydrique ou la sécheresse. Des valeurs élevées peuvent indiquer un manque d'eau. Température : Comme pour le NDWI, la température influence l'évapotranspiration et donc l'interprétation du NMMDI. Application à vos Clusters :

Cluster 2 (Mois 1,2, 3, 12 - Forte activité végétative) :

Hypothèses : On peut supposer que ces mois correspondent à une période avec des températures optimales pour la croissance, une humidité suffisante et une disponibilité adéquate en nutriments. Mesures à vérifier : Il faudrait examiner les données de température, d'humidité et les analyses de sol (M, P, K) pour ces mois afin de confirmer ces hypothèses. On s'attendrait à trouver des températures modérées à chaudes, une humidité du sol relativement élevée et des niveaux de nutriments ni trop bas ni trop élevés. Cluster 3 (Mois 7, 8 - Stress hydrique potentiel) :

Hypothèses : On peut supposer que ces mois correspondent à une période de sécheresse ou de stress hydrique, potentiellement associée à des températures élevées. Mesures à vérifier : Il faudrait examiner les données de température et d'humidité pour ces mois. On s'attendrait à trouver des températures potentiellement élevées et une faible humidité du sol. Des analyses de sol pourraient également révéler des carences en nutriments qui auraient pu exacerber les effets du stress hydrique. Cluster 1 (Mois 4,5, 6, 9, 10 - Transition/Valeurs moyennes) :

Hypothèses : Ces mois pourraient correspondre à des périodes de transition entre des conditions favorables et défavorables à la croissance, ou à des conditions plus modérées. Mesures à vérifier : Il faudrait examiner les données de température, d'humidité et de nutriments pour ces mois afin de comprendre les facteurs qui limitent l'activité végétative par rapport au Cluster 2.

Méthodologie :

Pour établir des liens concrets, vous devez collecter des données sur la température, l'humidité et les nutriments du sol pour les mêmes périodes que vos données d'indices de végétation. Ensuite

Régressions : Modéliser les relations entre les indices et les variables environnementales et les nutriments à l'aide de modèles de régression.

1. ***Cluster 0:***


*   Nombre de mois: 4
*   Mois: [1, 2, 3, 12]
*   **2** est le mois le plus proche du centroïde

2. ***Cluster 1***:
*   Nombre de mois: 6
* Mois: [4, 5, 6, 9, 10, 11]
* 5 est le mois le plus proche du centroïde
3. ***Cluster 2:***

* Nombre de mois: 2
* Mois: [7, 8]
* 7 est le mois le plus proche du centroïde

### *Partie 3*

besoin de
1. un fichier de donnees de tarrain ***terrain_N045_or.csv***
2. un fichier geojson de mon terrain d etude ***polygon3.geojson***
"""

# =====================================
# 1. Charger et Prétraiter les Données de Terrain
# =====================================

# Objectif : Charger les données terrain, vérifier leur qualité, et les préparer pour l'analyse.

# Charger le dataset
file_path = "/content/terrain_N045_or.csv"
terrain_data = pd.read_csv(file_path)

# Vérifier le contenu et la structure
print("Aperçu des données terrain :")
print(terrain_data.head())
print(terrain_data.info())

# Supprimer les valeurs manquantes
terrain_data = terrain_data.dropna()

# Supprimer les doublons
terrain_data = terrain_data.drop_duplicates()
print("Données terrain nettoyées :")
print(terrain_data.info())

# =====================================
# 2. Charger et Préparer les Données Spatiales
# =====================================

# Objectif : Charger une région d'intérêt à partir d'un fichier GeoJSON et initialiser l'API Google Earth Engine.

# Initialiser l'API Earth Engine
ee.Authenticate()
ee.Initialize(project='ee-wissalouh10')

# Charger le fichier GeoJSON et extraire la géométrie
geojson_file = "/content/polygon3.geojson"
with open(geojson_file) as f:
    geojson_data = geojson.load(f)

geometry = geojson_data['features'][0]['geometry']
coordinates = geometry['coordinates']
roi = ee.Geometry.Polygon(coordinates)

print("Région d'intérêt définie :", roi.getInfo())

# =====================================
# 3. Récupérer et Enrichir l'Image Sentinel-2
# =====================================

# Objectif : Récupérer une image Sentinel-2 avec une faible couverture nuageuse et calculer les indices spectraux.

# Fonction pour récupérer une image Sentinel-2 avec la couverture nuageuse minimale
def get_clear_sentinel2_image(start_date, end_date, roi):
    sentinel2 = ee.ImageCollection("COPERNICUS/S2")
    filtered_collection = sentinel2.filterBounds(roi).filterDate(start_date, end_date)
    sorted_collection = filtered_collection.sort('CLOUDY_PIXEL_PERCENTAGE')
    image = sorted_collection.first()
    if not image:
        raise ValueError("No image found within la période donnée et la région.")
    image_roi = image.clip(roi)
    return image_roi

# Fonction pour ajouter les indices spectraux
def calculate_and_add_index(image):
    """Calcule et ajoute les indices spectraux à l'image."""

    def get_ndvi(image):
        """Calcule l'indice NDVI."""
        return image.normalizedDifference(['B8', 'B4']).rename('NDVI')

    def get_ndwi(image):
        """Calcule l'indice NDWI."""
        return image.normalizedDifference(['B3', 'B8']).rename('NDWI')

    def get_mndwi(image):
        """Calcule l'indice MNDWI."""
        return image.normalizedDifference(['B3', 'B11']).rename('MNDWI')

    def get_ndre(image):
        """Calcule l'indice NDRE."""
        return image.normalizedDifference(['B8A', 'B5']).rename('NDRE')

    def get_pri(image):
        """Calcule l'indice PRI."""
        return image.expression(
            '(B5 - B6) / (B5 + B6)',
            {'B5': image.select('B5'), 'B6': image.select('B6')}).rename('PRI')

    def get_cire(image):
        """Calcule l'indice CIRE."""
        return image.expression(
            '(B8A / B5) - 1',
            {'B8A': image.select('B8A'), 'B5': image.select('B5')}).rename('CIRE')

    def get_gndvi(image):
        """Calcule l'indice GNDVI."""
        return image.normalizedDifference(['B8', 'B3']).rename('GNDVI')

    def get_sipi(image):
        """Calcule l'indice SIPI."""
        return image.expression(
            '(B8 - B4) / (B8 - B2)',
            {'B8': image.select('B8'), 'B4': image.select('B4'), 'B2': image.select('B2')}).rename('SIPI')

    def get_lst(image):
      """Calcule LST (Land Surface Temperature) - exemple simplifié"""
      return image.select('B10').multiply(0.1).rename('LST')

    def get_nmdi(image):
        """Calcule l'indice NMDI."""
        return image.normalizedDifference(['B11', 'B12']).rename('NMDI')


    image = image.addBands(get_ndvi(image)) \
                  .addBands(get_ndwi(image)) \
                  .addBands(get_mndwi(image)) \
                  .addBands(get_ndre(image)) \
                  .addBands(get_pri(image)) \
                  .addBands(get_cire(image)) \
                  .addBands(get_gndvi(image)) \
                  .addBands(get_sipi(image)) \
                  .addBands(get_lst(image)) \
                  .addBands(get_nmdi(image))
    return image

# Définir les dates
start_date = '2024-5-18'
end_date = '2024-5-25'

# Récupérer l'image Sentinel-2 et ajouter les indices
try:
    sentinel_image = get_clear_sentinel2_image(start_date, end_date, roi)
    indexed_image = calculate_and_add_index(sentinel_image)
    print("Image Sentinel-2 récupérée et indices ajoutés avec succès.")
except ValueError as e:
    print("Erreur :", e)

# =====================================
# 4. Extraire et Enregistrer les Valeurs de Pixels
# =====================================

# Objectif : Extraire les valeurs des indices spectraux et les coordonnées géographiques des pixels.

def extract_pixel_values(image, roi, scale=10):
    """
    Extrait les valeurs des pixels d'une image dans une région d'intérêt
    en incluant uniquement les indices spectraux et les coordonnées.
    """
    # Limiter les bandes aux indices d'intérêt
    bands = ['NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']
    sampled_points = image.select(bands).sample(
        region=roi,
        scale=scale,
        geometries=True  # Inclure les coordonnées géographiques
    )

    # Convertir les données échantillonnées en un format lisible
    features = sampled_points.getInfo()['features']
    spectral_data = []
    for feature in features:
        coords = feature['geometry']['coordinates']  # Longitude et Latitude
        properties = feature['properties']  # Valeurs des indices spectraux
        # Ajouter les coordonnées aux propriétés
        properties['longitude'] = coords[0]
        properties['latitude'] = coords[1]
        spectral_data.append(properties)

    return spectral_data

# Appeler la fonction pour extraire les valeurs
try:
    # Appliquez les indices à l'image
    indexed_image = calculate_and_add_index(sentinel_image)
    pixel_data = extract_pixel_values(indexed_image, roi, scale=10)
    print(f"Nombre de pixels extraits : {len(pixel_data)}")
except Exception as e:
    print("Erreur :", e)

# Convertir les données en DataFrame
df = pd.DataFrame(pixel_data)

# Enregistrer dans un fichier CSV
csv_filename = "pixel_values.csv"
df.to_csv(csv_filename, index=False)
print(f"Fichier CSV enregistré sous : {csv_filename}")

# Analyse des données avec Pandas
# Charger le fichier CSV
csv_file="/content/pixel_values.csv"
spectral_data = pd.read_csv(csv_file)

# Afficher les premières lignes
print("Premières lignes des données :")
print(spectral_data.head())

# Infos sur le dataset
print("Infos sur le dataset :")
print(spectral_data.info())

# Vérification des valeurs manquantes
missing_values = spectral_data.isnull().sum()
print("Valeurs manquantes dans les colonnes :")
print(missing_values)

# Renommer les colonnes si nécessaire
terrain_data.rename(columns={ 'latitude':'lat_terrain',  'longitude':'long_terrain'}, inplace=True)
spectral_data.rename(columns={'latitude':'lat_spectral' ,  'longitude':'long_spectral'}, inplace=True)

# Définir la résolution de la grille (par exemple, 10 mètres)
resolution = 0.0001  # Cela dépend de système de projection (exemple pour degrés).

#j ai ajouter pour chaque dateset du terrain et du spectral les colonnes de pixel10 lat et log pour avoire les memes coordonnees dans les deux datea set pour les pouvoire fusionneer
# Calculer les indices de pixels pour les données terrain
terrain_data['pixel10_lat'] = (terrain_data['lat_terrain'] // resolution).astype(int)
terrain_data['pixel10_lon'] = (terrain_data['long_terrain'] // resolution).astype(int)

# Grouper les points ayant les mêmes indices de pixel et calculer la moyenne de l'humidité
grouped_terrain = terrain_data.groupby(['pixel10_lat', 'pixel10_lon'], as_index=False).agg({
    'lat_terrain': 'mean',  # Optionnel : Moyenne des coordonnées des points du groupe
    'long_terrain': 'mean',  # Optionnel : Moyenne des coordonnées des points du groupe
    'N':'mean',
    'P':'mean',
    'K':'mean',
    'temperature':'mean',
    'humidity': 'mean',  # Moyenne de l'humidité
    'rainfall':'mean',
    'pH':'mean'
})
# Calculer les indices de pixels pour les données spectrales
spectral_data['pixel10_lat'] = (spectral_data['lat_spectral'] // resolution).astype(int)
spectral_data['pixel10_lon'] = (spectral_data['long_spectral'] // resolution).astype(int)

"""Avantages de cette méthode : Précision : Les points proches sont regroupés dans des pixels selon la résolution souhaitée. Conservation des données : Aucun point n'est ignoré ; les humidités sont moyennées. Flexibilité : Facilement adaptable à des résolutions différentes (10m, 20m, etc.)."""

# Fusionner les données terrain agrégées avec les données spectrales sur les indices de pixels
merged_data = pd.merge(
    grouped_terrain,
    spectral_data,
    on=['pixel10_lat', 'pixel10_lon'],
    how='inner'
)

print("Données fusionnées :")
print(merged_data)

# Afficher les premières lignes du dataset pour examiner la structure
print(merged_data.head())

# Vérifier les valeurs manquantes
print(merged_data.isnull().sum())

# Suppression des colonnes inutiles
merged_data = merged_data.drop(merged_data.columns[[3, 2, 22,21]], axis=1)

# Afficher des statistiques descriptives
print(merged_data.describe())

# Suppression des colonnes inutiles pour l analyse
df= merged_data.drop(merged_data.columns[[0, 1]], axis=1)

# Sauvegarder les données fusionnees dans un nouveau fichier CSV
df.to_csv("terrain&spectral_data_NOgeol_for_regr.csv", index=False)
df = pd.read_csv("/content/terrain&spectral_data_NOgeol_for_regre.csv")

# Charger les données
data3 =df

# Afficher les premières lignes pour vérifier les colonnes
print("Colonnes disponibles :", data3.columns)

# Identifiez les colonnes des variables environnementales et des indices
variables_env = ['N', 'P', 'K', 'temperature', 'humidity', 'rainfall', 'pH']
indices = ['CIRE', 'GNDVI', 'LST', 'MNDWI', 'NDRE', 'NDVI', 'NDWI', 'NMDI', 'PRI', 'SIPI']

# Vérifiez si toutes les colonnes nécessaires sont présentes
if not set(variables_env).issubset(data3.columns) or not set(indices).issubset(data3.columns):
    raise ValueError("Certaines colonnes nécessaires sont manquantes dans les données.")

# Extraire les variables environnementales et les indices
env_data = data3[variables_env]
indices_data = data3[indices]

# Calculer la matrice de corrélation entre les variables environnementales et les indices
correlation_matrix = pd.DataFrame(
    {var: indices_data.corrwith(env_data[var]) for var in variables_env}
)
# Transposer pour une meilleure lisibilité
correlation_matrix = correlation_matrix.T
correlation_matrix.index = variables_env
print("Matrice de corrélation :")
print(correlation_matrix)

# Tracer une heatmap des corrélations
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', xticklabels=indices, yticklabels=variables_env)
plt.title('Corrélation entre les variables environnementales et les indices')
plt.xlabel('Indices')
plt.ylabel('Variables environnementales')
plt.show()

"""rainfall est static pour une zone donnée"""

# Sauvegarder la matrice de corrélation
correlation_output_path = 'correlation_results.csv'
correlation_matrix.to_csv(correlation_output_path, index=True)
print(f"Matrice de corrélation enregistrée dans {correlation_output_path}")

def regression_for_mesure(df, x, y):
    """
    Effectue une régression linéaire (simple ou multiple) et retourne l'équation et les métriques.

    Args:
        df: DataFrame contenant les données.
        x: Nom de la colonne (string) représentant la variable dépendante.
        y: Liste des noms de colonnes (list de strings) représentant les variables indépendantes.

    Returns:
        Un dictionnaire avec l'équation, le R² et les métriques (MSE, MAE), ou None en cas d'erreur.
    """
    try:
        # Validation des colonnes
        if not set([x] + y).issubset(df.columns):
            raise KeyError("Certaines colonnes ne sont pas présentes dans le DataFrame.")

        X = df[y]  # Variables indépendantes
        Y = df[[x]]  # Variable dépendante

        model = LinearRegression()
        model.fit(X, Y)

        # Construire l'équation
        equation = f"{x} = " + " , ".join([f"{model.coef_[0][i]:.4f}" for i, var in enumerate(y)])
        equation += f" + {model.intercept_[0]:.4f}"


        # Calcul des prédictions et des métriques
        predictions = model.predict(X)
        r2 = r2_score(Y, predictions)
        mse = mean_squared_error(Y, predictions)
        mae = mean_absolute_error(Y, predictions)

        # Résultats
        return {
            "equation": equation,
            "R2": r2,
            "MSE": mse,
            "MAE": mae
        }

    except KeyError as e:
        print(f"Erreur : {e}")
    except ValueError as e:
        print(f"Erreur de valeur : {e}")
    except Exception as e:
        print(f"Erreur inattendue : {e}")

    return None

dependent_vars = ['N', 'P', 'K', 'temperature', 'humidity', 'pH', 'rainfall']
independent_vars = ['NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']

for dep_var in dependent_vars:
    result = regression_for_mesure(data3, dep_var, independent_vars)
    if result:
        print(f"Equation pour {dep_var} : {result['equation']}")
        print(f"R² : {result['R2']:.4f}, MSE : {result['MSE']:.4f}, MAE : {result['MAE']:.4f}")

# Charger les données
df_mois_path = '/content/description_par_mois.csv'
df_mois = pd.read_csv(df_mois_path)

# Identifier les colonnes des moyennes des indices
mean_columns = [col for col in df_mois.columns if '.1' in col]

# Réduire le DataFrame aux colonnes moyennes
df_mois_reduit = df_mois[mean_columns].copy()

# Correspondance des indices
indices = ['NDVI', 'NDWI', 'MNDWI', 'NDRE', 'PRI', 'CIRE', 'GNDVI', 'SIPI', 'LST', 'NMDI']
if len(df_mois_reduit.columns) == len(indices):
    df_mois_reduit.columns = indices
else:
    raise ValueError("Le nombre de colonnes dans df_mois_reduit ne correspond pas au nombre d'indices.")

# Nettoyer et convertir les données
df_mois_reduit = df_mois_reduit.drop(index=0).reset_index(drop=True).apply(pd.to_numeric, errors='coerce')

# Vérifier les valeurs manquantes
if df_mois_reduit.isna().sum().sum() > 0:
    print("Attention : des valeurs manquantes ont été détectées.")

import pandas as pd
from tabulate import tabulate  # Pour afficher les résultats sous forme de tableau

# Dictionnaire des coefficients et intercepts des régressions précédentes
regression_coeffs = {
    'N': {
        'coeffs': [0.0746, -0.0105, -0.0018, 0.0430, 0.0507, 0.0258, 0.0105, -0.0072, 0.0027, -0.0008],
        'intercept': 45.1388,
        'R²': 0.8357,
        'MSE': 0.0001,
        'MAE': 0.0079
    },
    'P': {
        'coeffs': [-0.1000, -1.2248, 0.0086, 0.0536, 0.1046, 0.0482, 1.2248, 0.0359, -0.0085, -0.0279],
        'intercept': 44.7479,
        'R²': 0.9997,
        'MSE': 0.0001,
        'MAE': 0.0084
    },
    'K': {
        'coeffs': [-0.0181, -0.0040, 0.6789, 0.0868, 0.0095, -0.1112, 0.0040, 0.0283, 0.0038, 0.0047],
        'intercept': 35.2831,
        'R²': 0.9968,
        'MSE': 0.0001,
        'MAE': 0.0076
    },
    'temperature': {
        'coeffs': [0.0182, 0.0035, 0.6174, 0.27, 0.1903, 0.011, 0.0035, 0.0364, 0.0443, 0.0104],
        'intercept': 7.9628,
        'R²': 0.5160,
        'MSE': 0.0001,
        'MAE': 0.0080
    },
    'humidity': {
        'coeffs': [-0.0212, 1.0793, -0.0055, -0.0884, -0.0332, 0.0313, -1.0793, 0.0289, -0.0018, 0.0376],
        'intercept': 77.4380,
        'R²': 0.9997,
        'MSE': 0.0001,
        'MAE': 0.0070
    },
    'pH': {
        'coeffs': [-0.0330, -0.0138, -0.0019, -0.5693, 0.0139, 0.0463, 0.0138, 0.0081, -0.0052, -0.0251],
        'intercept': 6.8077,
        'R²': 0.9927,
        'MSE': 0.0001,
        'MAE': 0.0079
    },
    'rainfall': {
        'coeffs': [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        'intercept': 100.0000,
        'R²': 1.0000,
        'MSE': 0.0000,
        'MAE': 0.0000
    }
}

# Calcul des variables environnementales
def calculate_environmental_variables(df, indices, coeffs_dict):
    environmental_values = {}
    for variable, params in coeffs_dict.items():
        coeffs = params['coeffs']
        intercept = params['intercept']

        # Vérifier que le DataFrame contient toutes les colonnes nécessaires
        if not set(indices).issubset(df.columns):
            raise ValueError(f"Les indices {indices} ne sont pas tous présents dans le DataFrame.")

        # Calculer chaque ligne comme une combinaison linéaire des indices
        environmental_values[variable] = df[indices].dot(coeffs) + intercept

    return pd.DataFrame(environmental_values)

environmental_data=calculate_environmental_variables(df_mois_reduit,indices,regression_coeffs)

# Résultats de la Partie 2 : Clusters et Mois associés
clusters = {
    0: {"months": [1, 2, 3, 12], "closest_month": 2},
    1: {"months": [4, 5, 6, 9, 10, 11], "closest_month": 5},
    2: {"months": [7, 8], "closest_month": 7}
}

# Résultats de la Partie 3 : Variables environnementales pour chaque mois
environmental_data = pd.DataFrame({
    'month': list(range(1, 13)),
    'N': [48.38, 50.78, 51.45, 49.82, 50.53, 51.59, 48.77, 48.03, 49.17, 49.37, 49.63, 49.55],
    'P': [46.98, 47.99, 48.25, 47.39, 47.69, 48.11, 47.05, 46.79, 47.12, 47.25, 47.41, 47.39],
    'K': [34.07, 33.14, 32.87, 33.44, 33.08, 32.68, 33.71, 34.02, 33.62, 33.56, 33.51, 33.59],
    'temperature': [23.92, 23.31, 24.65, 29.06, 29.57, 33.86, 34.08, 31.15, 30.65, 28.59, 25.15, 23.81],
    'humidity': [51.41, 54.98, 56.01, 53.80, 54.87, 56.47, 52.16, 50.98, 52.83, 53.09, 53.47, 53.24],
    'pH': [6.75, 6.90, 6.96, 6.95, 6.99, 7.06, 6.86, 6.81, 6.90, 6.90, 6.85, 6.84],
    'rainfall': [68.0] * 12
})


# Résultats de la Partie 2 : Clusters et Mois associés
clusters = {
    0: {"months": [1, 2, 3, 12], "closest_month": 2},
    1: {"months": [4, 5, 6, 9, 10, 11], "closest_month": 5},
    2: {"months": [7, 8], "closest_month": 7}
}

# Étape 1 : Extraire les données pour les mois les plus proches du centroïde
selected_months = [cluster["closest_month"] for cluster in clusters.values()]
selected_data = environmental_data[environmental_data['month'].isin(selected_months)]

# Étape 2 : Ajouter une colonne cluster aux données sélectionnées
def get_cluster_by_month(month):
    for cluster_id, cluster_info in clusters.items():
        if month == cluster_info["closest_month"]:
            return cluster_id
    return None

selected_data['cluster'] = selected_data['month'].apply(get_cluster_by_month)

# Prédictions avec le modèle pour chaque cluster
cluster_predictions = {}
for _, row in selected_data.iterrows():
    cluster_id = row['cluster']
    cluster_data = row[['N', 'P', 'K', 'temperature', 'humidity', 'pH', 'rainfall']].values
    sample_input = cluster_data.reshape(1, -1)  # Reshape pour le modèle
    prediction = model.predict(sample_input)  # Prédire avec le modèle
    cluster_predictions[cluster_id] = prediction[0]

# Afficher les résultats sous forme de tableau
result_data = []
for cluster_id, cluster_info in clusters.items():
    closest_month = cluster_info["closest_month"]
    row_data = selected_data[selected_data['month'] == closest_month].iloc[0]
    row = {
        "Cluster": cluster_id,
        "Months": ", ".join(map(str, cluster_info["months"])),
        "Closest Month": closest_month,
        "Suggested Crop": cluster_predictions[cluster_id],
        "N": f"{row_data['N']:.2f}",
        "P": f"{row_data['P']:.2f}",
        "K": f"{row_data['K']:.2f}",
        "Temperature": f"{row_data['temperature']:.2f}",
        "Humidity": f"{row_data['humidity']:.2f}",
        "pH": f"{row_data['pH']:.2f}",
        "Rainfall": f"{row_data['rainfall']:.2f}"
    }
    result_data.append(row)

# Affichage en tableau formaté
print("\nFinal Results:")
print(tabulate(result_data, headers="keys", tablefmt="fancy_grid"))
